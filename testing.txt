START:
sudo systemctl start kafka

/opt/spark/sbin/start-all.sh

CREATE TOPIC:
/usr/local/kafka/bin/kafka-topics.sh --create --topic reddit-posts --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1


POSTGRES:
ravani@sravani-VirtualBox:~$ psql -U root -W -d reddit_stream_db -h localhost
Password: 
psql (14.17 (Ubuntu 14.17-0ubuntu0.22.04.1))
SSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off)
Type "help" for help.

reddit_stream_db=> DO
$$
DECLARE
    r RECORD;
BEGIN
    FOR r IN (SELECT tablename FROM pg_tables WHERE schemaname = 'public') LOOP
        EXECUTE 'DROP TABLE IF EXISTS ' || quote_ident(r.tablename) || ' CASCADE';
    END LOOP;
END;
$$;
DO
reddit_stream_db=> CREATE TABLE sentiment_results (
    id TEXT PRIMARY KEY,
    title TEXT,
    selftext TEXT,
    score INTEGER,
    created_utc TIMESTAMP,
    num_comments INTEGER,
    sentiment TEXT
);
CREATE TABLE


RUNNING:
python3 ingestion/reddit_producer.py

CHECKING:
python3 ingestion/cons.py

SPARK:
 /home/sravani/.local/bin/spark-submit     --jars /home/sravani/jars/postgresql-42.3.1.jar     --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1     spark_streaming/spark_sentiment_consumer.py

 fix jar file location in command and in code to local one


 CHECKING POSTGRES:
 reddit_stream_db=> \x
Expanded display is on.
reddit_stream_db=> SELECT * FROM sentiment_results ORDER BY created_utc DESC LIMIT 10;


 /home/sravani/.local/bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1   /home/sravani/dbt/DBT-PROJECT/spark_streaming/keyword_filter.py


 /home/sravani/.local/bin/spark-submit   --jars /home/sravani/jars/postgresql-42.3.1.jar   --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0   /home/sravani/dbt/DBT-PROJECT/spark_streaming/spark_sentiment_consumer.py


from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.functions import explode, split, col, lower, window
from pyspark.sql import DataFrame
import logging

# Initialize Spark Session and set log level
spark = SparkSession.builder \
    .appName("RedditWordCount") \
    .getOrCreate()

# Set Spark log level to WARN
spark.sparkContext.setLogLevel("WARN")

# Setup Kafka Consumer
kafka_bootstrap_servers = 'localhost:9092'
kafka_topic = 'hot-topic'

# Load Kafka Stream as a DataFrame
kafka_df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_bootstrap_servers) \
    .option("subscribe", kafka_topic) \
    .load()

# Parse Kafka JSON data
parsed_df = kafka_df.selectExpr("CAST(value AS STRING)", "timestamp as kafka_timestamp") \
    .select(F.from_json("value", "STRUCT<id: STRING, title: STRING, score: INT, created_utc: DOUBLE, num_comments: INT, selftext: STRING>").alias("data"),
            "kafka_timestamp")

# Extract title column
titles_df = parsed_df.select("data.title", "kafka_timestamp")

# Tokenize titles into words
words_df = titles_df.select(explode(split(lower(col("title")), r"\W+")).alias("word"), "kafka_timestamp")

# Remove empty words (if any)
words_df = words_df.filter(words_df["word"] != "")

# Perform windowed aggregation using the Kafka timestamp
windowed_df = words_df \
    .groupBy(window(col("kafka_timestamp"), "1 minute"), "word") \
    .count()

# Extract start and end from the window column and rename them
windowed_df = windowed_df \
    .withColumn("window_start", col("window.start")) \
    .withColumn("window_end", col("window.end")) \
    .drop("window")  # Drop the original window column

# Show schema to debug
windowed_df.printSchema()

# Write to PostgreSQL function using JDBC
def write_to_postgres(df: DataFrame, epoch_id: int):
    # Print the batch data to debug
    df.show(truncate=False)  # Print out the data
    
    # Writing to PostgreSQL using JDBC
    df.write.format("jdbc") \
        .option("url", "jdbc:postgresql://localhost:5432/reddit_stream_db") \
        .option("dbtable", "trending_words") \
        .option("user", "root") \
        .option("password", "root") \
        .option("driver", "org.postgresql.Driver") \
        .mode("append") \
        .save()

# Write the word count to PostgreSQL
query = windowed_df.writeStream \
    .foreachBatch(write_to_postgres) \
    .outputMode("complete") \
    .start()

query.awaitTermination()

# Stop the Spark session gracefully
spark.stop()


 /home/sravani/.local/bin/spark-submit   --jars /home/sravani/jars/postgresql-42.3.1.jar   --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1   /home/sravani/dbt/DBT-PROJECT/spark_streaming/hot_topic_aggregator.py


 CREATE TABLE trending_words_batch (
    batch_id INT,
    word VARCHAR(255),
    count INT,
    window_start TIMESTAMP,
    window_end TIMESTAMP,
    PRIMARY KEY (batch_id, word)
);
